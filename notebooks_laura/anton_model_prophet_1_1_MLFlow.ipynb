{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0db2e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# Get the parent directory and add it to sys.path\n",
    "parent_dir = os.path.abspath(\"..\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from prophet import Prophet\n",
    "from functions_anton.functions_models import plot_forecast, step_modification_to_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8ce512",
   "metadata": {},
   "source": [
    "# MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Get absolute path for MLflow directory\n",
    "mlflow_dir = os.path.abspath(\"../data/mlflow\")\n",
    "\n",
    "# Create fresh directory\n",
    "os.makedirs(mlflow_dir, exist_ok=True)\n",
    "\n",
    "# Basic MLflow config - use file protocol with absolute path \n",
    "mlflow.set_tracking_uri(f\"file://{os.path.abspath(mlflow_dir)}\")\n",
    "\n",
    "# Set up FuelProphet experiment\n",
    "try:\n",
    "    experiment_name = \"FuelProphet_Anton_Loop_1\"\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    \n",
    "    if experiment is None:\n",
    "        # Only create if it doesn't exist\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        \n",
    "    # Set as active experiment\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(\"FuelProphet experiment setup successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"MLflow error: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.prophet.log_model(pr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d944b5",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba923f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "df01_train = pd.read_parquet('../data/parquet_4_testing_2/train_01.parquet')\n",
    "df02_train = pd.read_parquet('../data/parquet_4_testing_2/train_02.parquet')\n",
    "df03_train = pd.read_parquet('../data/parquet_4_testing_2/train_03.parquet')\n",
    "df04_train = pd.read_parquet('../data/parquet_4_testing_2/train_04.parquet')\n",
    "df05_train = pd.read_parquet('../data/parquet_4_testing_2/train_05.parquet')\n",
    "df06_train = pd.read_parquet('../data/parquet_4_testing_2/train_06.parquet')\n",
    "df07_train = pd.read_parquet('../data/parquet_4_testing_2/train_07.parquet')\n",
    "df08_train = pd.read_parquet('../data/parquet_4_testing_2/train_08.parquet')\n",
    "df09_train = pd.read_parquet('../data/parquet_4_testing_2/train_09.parquet')\n",
    "df10_train = pd.read_parquet('../data/parquet_4_testing_2/train_10.parquet')\n",
    "df11_train = pd.read_parquet('../data/parquet_4_testing_2/train_11.parquet')\n",
    "df12_train = pd.read_parquet('../data/parquet_4_testing_2/train_12.parquet')\n",
    "df13_train = pd.read_parquet('../data/parquet_4_testing_2/train_13.parquet')\n",
    "df14_train = pd.read_parquet('../data/parquet_4_testing_2/train_14.parquet')\n",
    "df15_train = pd.read_parquet('../data/parquet_4_testing_2/train_15.parquet')\n",
    "df16_train = pd.read_parquet('../data/parquet_4_testing_2/train_16.parquet')\n",
    "df17_train = pd.read_parquet('../data/parquet_4_testing_2/train_17.parquet')\n",
    "df18_train = pd.read_parquet('../data/parquet_4_testing_2/train_18.parquet')\n",
    "df19_train = pd.read_parquet('../data/parquet_4_testing_2/train_19.parquet')\n",
    "df20_train = pd.read_parquet('../data/parquet_4_testing_2/train_20.parquet')\n",
    "df21_train = pd.read_parquet('../data/parquet_4_testing_2/train_21.parquet')\n",
    "df22_train = pd.read_parquet('../data/parquet_4_testing_2/train_22.parquet')\n",
    "df23_train = pd.read_parquet('../data/parquet_4_testing_2/train_23.parquet')\n",
    "df24_train = pd.read_parquet('../data/parquet_4_testing_2/train_24.parquet')\n",
    "df25_train = pd.read_parquet('../data/parquet_4_testing_2/train_25.parquet')\n",
    "df26_train = pd.read_parquet('../data/parquet_4_testing_2/train_26.parquet')\n",
    "df27_train = pd.read_parquet('../data/parquet_4_testing_2/train_27.parquet')\n",
    "df28_train = pd.read_parquet('../data/parquet_4_testing_2/train_28.parquet')\n",
    "df29_train = pd.read_parquet('../data/parquet_4_testing_2/train_29.parquet')\n",
    "df30_train = pd.read_parquet('../data/parquet_4_testing_2/train_30.parquet')\n",
    "\n",
    "# Test\n",
    "df01_test = pd.read_parquet('../data/parquet_4_testing_2/test_01.parquet')\n",
    "df02_test = pd.read_parquet('../data/parquet_4_testing_2/test_02.parquet')\n",
    "df03_test = pd.read_parquet('../data/parquet_4_testing_2/test_03.parquet')\n",
    "df04_test = pd.read_parquet('../data/parquet_4_testing_2/test_04.parquet')\n",
    "df05_test = pd.read_parquet('../data/parquet_4_testing_2/test_05.parquet')\n",
    "df06_test = pd.read_parquet('../data/parquet_4_testing_2/test_06.parquet')\n",
    "df07_test = pd.read_parquet('../data/parquet_4_testing_2/test_07.parquet')\n",
    "df08_test = pd.read_parquet('../data/parquet_4_testing_2/test_08.parquet')\n",
    "df09_test = pd.read_parquet('../data/parquet_4_testing_2/test_09.parquet')\n",
    "df10_test = pd.read_parquet('../data/parquet_4_testing_2/test_10.parquet')\n",
    "df11_test = pd.read_parquet('../data/parquet_4_testing_2/test_11.parquet')\n",
    "df12_test = pd.read_parquet('../data/parquet_4_testing_2/test_12.parquet')\n",
    "df13_test = pd.read_parquet('../data/parquet_4_testing_2/test_13.parquet')\n",
    "df14_test = pd.read_parquet('../data/parquet_4_testing_2/test_14.parquet')\n",
    "df15_test = pd.read_parquet('../data/parquet_4_testing_2/test_15.parquet')\n",
    "df16_test = pd.read_parquet('../data/parquet_4_testing_2/test_16.parquet')\n",
    "df17_test = pd.read_parquet('../data/parquet_4_testing_2/test_17.parquet')\n",
    "df18_test = pd.read_parquet('../data/parquet_4_testing_2/test_18.parquet')\n",
    "df19_test = pd.read_parquet('../data/parquet_4_testing_2/test_19.parquet')\n",
    "df20_test = pd.read_parquet('../data/parquet_4_testing_2/test_20.parquet')\n",
    "df21_test = pd.read_parquet('../data/parquet_4_testing_2/test_21.parquet')\n",
    "df22_test = pd.read_parquet('../data/parquet_4_testing_2/test_22.parquet')\n",
    "df23_test = pd.read_parquet('../data/parquet_4_testing_2/test_23.parquet')\n",
    "df24_test = pd.read_parquet('../data/parquet_4_testing_2/test_24.parquet')\n",
    "df25_test = pd.read_parquet('../data/parquet_4_testing_2/test_25.parquet')\n",
    "df26_test = pd.read_parquet('../data/parquet_4_testing_2/test_26.parquet')\n",
    "df27_test = pd.read_parquet('../data/parquet_4_testing_2/test_27.parquet')\n",
    "df28_test = pd.read_parquet('../data/parquet_4_testing_2/test_28.parquet')\n",
    "df29_test = pd.read_parquet('../data/parquet_4_testing_2/test_29.parquet')\n",
    "df30_test = pd.read_parquet('../data/parquet_4_testing_2/test_30.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758ae702",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e09b1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_freq = 5 # minutes, dicretization between individual values of the forcast\n",
    "forecast_days = 7 # how many days should be forecasted\n",
    "train_days = 30 # how many days will be used to train on\n",
    "sampling_rate = 5 #in minutes, very important parameter\n",
    "\n",
    "train_start = pd.to_datetime(df_train['date'].iloc[-1]) - pd.Timedelta(days = train_days)\n",
    "train_end = pd.to_datetime(df_train['date'].iloc[-1])\n",
    "# The forecast starts immediately after the training data ends\n",
    "forecast_start_date = train_end + pd.Timedelta(minutes=forecast_freq)\n",
    "# The forecast extends for 'forecast_days' from its start\n",
    "forecast_end_date = forecast_start_date + pd.Timedelta(days=forecast_days) - pd.Timedelta(minutes=forecast_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2854d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an error log dictionary\n",
    "error_log = {}\n",
    "\n",
    "# LOOP\n",
    "for dataset_name, (train, test) in datasets.items():\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "    \n",
    "    try:\n",
    "        mlflow.end_run()\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"fuel_model_Anton_{dataset_name}\") as run:\n",
    "            mlflow.set_tag(\"dataset\", f'{dataset_name}')\n",
    "\n",
    "        # block required to resample the data to the desired frequency\n",
    "        #speeds up Prophet enormously\n",
    "        df_prophet = df_train[(df_train['date'] >= train_start) & (df_train['date'] <= train_end) ]\n",
    "        df_prophet.set_index('date', inplace = True)\n",
    "        df_prophet.sort_index(inplace = True)\n",
    "        df_prophet = df_prophet.asfreq(f'{sampling_rate}T')\n",
    "        df_prophet = df_prophet.asfreq(pd.infer_freq(df_prophet.index)) # the infer_freq function will automatically detect the frequency of the time series data\n",
    "        df_prophet = df_prophet.reset_index().rename(columns={'date': 'ds', 'e5': 'y'}) # Prophet requires certain column names for prediction\n",
    "        df_prophet = df_prophet[['ds', 'y']]\n",
    "\n",
    "\n",
    "        df_regressor = df_test[['date', 'e5']]\n",
    "        df_regressor = df_regressor.rename(columns={'date': 'ds', 'e5': 'y'}) # Prophet requires certain column names for prediction\n",
    "\n",
    "        df_regressor = pd.concat([df_prophet[['ds', 'y']],df_regressor]) # just to get also the visible predictions of the train set\n",
    "\n",
    "        #Prophet itself\n",
    "\n",
    "        m = Prophet(\n",
    "            growth='linear',\n",
    "\n",
    "            changepoint_prior_scale= 0.75,  # The higher the value, the more flexible the trend\n",
    "            n_changepoints = 100, # Number of changepoints in the changepoint_range by default 80% of the data. In our case should be way higher and somehow connected to the trainset length\n",
    "            changepoint_range = 0.85, # The percentage of the history in which the changepoints are allowed to be placed. Default is 0.8. Since in our case the recent data is very important, try to extend to 0.9 or even 1\n",
    "\n",
    "\n",
    "            seasonality_mode='additive', # in our case multiplicative should not be the better choice\n",
    "            daily_seasonality=False,\n",
    "            weekly_seasonality=False,\n",
    "            yearly_seasonality=False\n",
    "        )\n",
    "        m.add_seasonality(name='daily', period=1, fourier_order=17, prior_scale = 30) # fourier 30\n",
    "        m.add_seasonality(name='weekly', period= 7, fourier_order=20, prior_scale = 30) # fourier  40\n",
    "\n",
    "        m.fit(df_prophet)\n",
    "\n",
    "        forecast = m.predict(df_regressor)\n",
    "        forecast = step_modification_to_forecast(forecast, threshold = 0.01) # makes the prediction less nervous:)\n",
    "\n",
    "        mlflow.prophet.log_model(m)\n",
    "\n",
    "        # Plot\n",
    "\n",
    "        train_plot_start = pd.to_datetime(df_train['date'].iloc[-1]) - pd.Timedelta(days = 2)\n",
    "        train_plot_end = pd.to_datetime(df_train['date'].iloc[-1]) + pd.Timedelta(days = 1)\n",
    "        \n",
    "        train_dates_plot = (train_plot_start.strftime('%Y-%m-%d'), train_plot_end.strftime('%Y-%m-%d'))\n",
    "        test_dates_plot = (forecast_start_date.strftime('%Y-%m-%d'), forecast_end_date.strftime('%Y-%m-%d'))\n",
    "        \n",
    "        prophet_pred_plot = forecast[['ds', 'yhat']].copy()\n",
    "        prophet_pred_plot = prophet_pred_plot.rename(columns={'ds': 'date', 'yhat': 'e5'})\n",
    "        prophet_pred_plot['date'] = pd.to_datetime(prophet_pred_plot['date'])\n",
    "        \n",
    "        plot_forecast(df_train, df_test, None, prophet_pred_plot, prophet_pred_plot, train_dates_plot, test_dates_plot)\n",
    "        plt.xlim(pd.to_datetime(train_dates_plot[0]),pd.to_datetime(test_dates_plot[1]))\n",
    "\n",
    "        print(f\"Successfully processed {dataset_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "            # Log the error and continue with next dataset\n",
    "            error_message = f\"Error in {dataset_name}: {str(e)}\"\n",
    "            error_log[dataset_name] = error_message\n",
    "            print(error_message)\n",
    "\n",
    "            # Make sure to end the MLflow run if it failed\n",
    "            mlflow.end_run()\n",
    "            continue\n",
    "\n",
    "\n",
    "# After the loop, print summary of errors\n",
    "print(\"\\n=== Error Summary ===\")\n",
    "if error_log:\n",
    "    print(f\"Total errors: {len(error_log)}\")\n",
    "    for dataset, error in error_log.items():\n",
    "        print(f\"\\n{dataset}:\")\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"All datasets processed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
